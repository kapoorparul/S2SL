<!DOCTYPE html>
<html lang="en">
<head>
<br/>
<br/>
<title>Speech to SL</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0" charset="utf-8">
<!-- jQuery -->
<script src="http://code.jquery.com/jquery.min.js"></script>
<!-- Bootstrap -->
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
<!-- Bootstrap -->
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>
<link rel="stylesheet" type="text/css" href="http://people.eecs.berkeley.edu/~shiry/styles/main.css"/>
</head>
<body>

<div class="container">
<div class="row">
	<div class="col-md-12">
		<div class="page-header">
			<h1 class="text-center"><b>Towards Automatic Speech to Sign Language Generation</b></h1>
		</div>
	</div>
</div>
<br>
<div class="row"> <!-- names row-->
		&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
		<div class="col-md-2">
			<h5 class="text-center"><a href="https://www.linkedin.com/in/parul-kapoor-b5b421b1/" target="_blank">Parul Kapoor</a></h5>
		</div>
		<div class="col-md-2">
			<h5 class="text-center"><a href="https://rudrabha.github.io/" target="_blank">Rudrabha Mukhopadhyay</a></h5>
		</div>
		<div class="col-md-2">
			<h5 class="text-center"><a href="">Sindhu B Hegde</a></h5>
		</div>
		<div class="col-md-2">
			<h5 class="text-center"><a href="https://www.cse.iitk.ac.in/users/vinaypn/">Vinay Namboodiri</a></h5>
		</div>
		<div class="col-md-2">
			<h5 class="text-center"><a href="https://faculty.iiit.ac.in/~jawahar/">C.V. Jawahar</a></h5>
		</div>

</div>
<br>
<div class="row">
	<div class="col-md-3">
	</div>
	<div class="col-md-2">
		<h5 class="text-right">IIT Kanpur</h5>
	</div>
	<div class="col-md-2">
		<h5 class="text-right">IIIT Hyderabad</h5>
	</div>
	<div class="col-md-2">
		<h5 class="text-left">Univ. of Bath</h5>
	</div>
	<div class="col-md-3">
	</div>
</div>

<br>

<div class="row">
	<div class="col-md-12">
		<h5 class="text-center">Interspeech, 2021</h5>
	</div>
<div>

<br>

<div class="row">
	<div class="col-md-3">
		<h3 class="text-center">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </h3>
	</div>
	<div class="col-md-1">
		<h3 class="text-center"><a href="https://github.com/kapoorparul/Towards-Automatic-Speech-to-SL" target="_blank">[Code]</a></h3>
	</div>
	<div class="col-md-1">
		<h3 class="text-center">&nbsp; &nbsp; | &nbsp; &nbsp; </h3>
	</div>
	<div class="col-md-2">
		<h3 class="text-center"><a href="https://drive.google.com/file/d/1B12qxs9jCGc-2Ns_FLoxjaRH3wK-0x4a/view?usp=sharing" target="_blank" style="white-space: nowrap;">[Demo Video]</a></h3>
	</div>
	<div class="col-md-1">
		<h3 class="text-center">&nbsp; &nbsp; | &nbsp; &nbsp; </h3>
	</div>
	<div class="col-md-1">
		<h3 class="text-center"><a href="#" target="_blank">[Dataset]</a></h3>
	</div>
	<div class="col-md-3">
	</div>
<div>

<div class="row">
	<div class="col-md-1">
		<!--left margin column-->
	</div>

	<div class="col-md-10 text-center"> <!--main content column-->

	<p>
	    <figure class="figure">
	      <img src="images/banner.png" class="img-fluid mx-auto" alt="XXX">
	      <figcaption class="figure-caption">
	      	Previous approaches have only attempted to generate sign-language from the text level, we focus on directly converting speech segments into sign-language. Our work opens up several assistive technology applications and can help effectively communicate with people suffering from hearing loss.
	      	<!-- We propose a novel approach to enhance the speech by hallucinating the visual stream for any given noisy audio. In contrast to the existing audio-visual methods, our approach works even in the absence of a reliable visual stream, while also performing better than audio-only works in unconstrained conditions due to the assistance of generated lip movements. -->
	      </figcaption>
	    </figure>
	</p>

	<br>
    <h2>Abstract</h2>
     <br>
    <p class="text-justify">
    	We aim to solve the highly challenging task of generating continuous sign language videos solely from speech segments for the first time. Recent efforts in this space have focused on generating such videos from human-annotated text transcripts without considering other modalities. However, replacing speech with sign language proves to be a practical solution while communicating with people suffering from hearing loss. Therefore, we eliminate the need of using text as input and design techniques that work for more natural, continuous, freely uttered speech covering an extensive vocabulary. Since the current datasets are inadequate for generating sign language directly from speech, we collect and release the first Indian sign language dataset comprising speech-level annotations, text transcripts, and the corresponding sign-language videos. Next, we propose a multi-tasking transformer network trained to generate signer's poses from speech segments. With speech-to-text as an auxiliary task and an additional cross-modal discriminator, our model learns to generate  continuous sign pose sequences in an end-to-end manner. Extensive experiments and comparisons with other baselines demonstrate the effectiveness of our approach. We also conduct additional ablation studies to analyze the effect of different modules of our network. A demo video containing several results is attached to the supplementary material.

  <!-- In this work, we re-think the task of speech enhancement in unconstrained real-world environments. Current state-of-the-art methods use only the audio stream and are limited in their performance in a wide range of real-world noises. Recent works using lip movements as additional cues improve the quality of generated speech over ``audio-only" methods. But, these methods cannot be used for several applications where the visual stream is unreliable or completely absent. We propose a new paradigm for speech enhancement by exploiting recent breakthroughs in speech-driven lip synthesis. Using one such model as a teacher network, we train a robust student network to produce accurate lip movements that mask away the noise, thus acting as a ``visual noise filter". The intelligibility of the speech enhanced by our pseudo-lip approach is almost close (< 3\% difference) to the case of using real lips. This implies that we can exploit the advantages of using lip movements even in the absence of a real video stream. We rigorously evaluate our model using quantitative metrics as well as qualitative human evaluations. Additional ablation studies and a demo video in the supplementary material containing qualitative comparisons and results clearly illustrate the effectiveness of our approach.  -->
    </p>

	<br>
 	<hr>
 	<br>

 <h2>Paper</h2>
  <br>
    <ul class="media-list, citations">
    <li class="media" id="gestures">
        <a class="pull-left" href="#">
            <img class="media-object img-fluid img-thumbnail mr-4" src="images/paper.png" alt="Speech2Lip Paper" width="150">
        </a>
        <div class="media-body">
            <h5 class="media-heading text-left">Towards Automatic Speech to Sign Language Generation</h5>
            <p class="text-left">
              Parul Kapoor, Rudrabha Mukhopadhyay, Sindhu Hegde Vinay Namboodiri and C.V. Jawahar
              <br>
              <span class="cite-title">Towards Automatic Speech to Sign Language Generation</span>,
              Interspeech, 2021.
              <br>
              <a href="https://arxiv.org/abs/2106.12790">[PDF]</a> | <a href="#gestures" role="button" data-toggle="collapse" data-target="#collapse-gestures" aria-expanded="false" aria-controls="collapse">[BibTeX]</a>
              <div class="collapse" id="collapse-gestures">
              <div class="card text-left bg-light mb-4">

              </div>
              </div>
            </p>
        </div>
    </li>
  </ul>

    <br>
 	<!-- <hr> -->
 	<!-- <br> -->

 	<!-- <h2>Demo Video</h2>
 	--- COMING SOON ---


    <hr>
    <br>

    <h2>Dataset</h2>
 	--- COMING SOON --- -->


    <hr>
    <br>
     <h2>Contact</h2>
     <br>
      <ul>
      	<li>Parul Kapoor - kparul@iitk.ac.in</li>
        <li>Rudrabha Mukhopadhyay - radrabha.m@research.iiit.ac.in</li>
        <li>Sindhu Hegde - sindhu.hegde@research.iiit.ac.in</li>
      </ul> 

    </div>
   	 

    <br>
 	<hr>
 	<br>


	</div> <!-- close middle column -->
	<div class="col-md-1">
	<!-- empty room saver on right -->
	</div>
</div> <!-- close main row -->
</div> <!-- close body container --> 
<footer>
  <br/>
  <br/>
</footer>
</div>
</body>
</html>
